{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "m5xfGFYrYjGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk, random, math\n",
        "from collections import Counter, defaultdict\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Download resources if first time\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "STEM = False\n",
        "LEMMATIZE = True\n",
        "REMOVE_STOPWORDS = False\n",
        "USE_UNK = True\n",
        "UNK_THRESH = 3\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "text = gutenberg.raw('melville-moby_dick.txt')\n",
        "sentences = sent_tokenize(text)\n",
        "import string\n",
        "\n",
        "def clean_tokenize(sent):\n",
        "    tokens = word_tokenize(sent.lower())\n",
        "    processed = []\n",
        "    for t in tokens:\n",
        "        if t in string.punctuation:\n",
        "            continue\n",
        "        if REMOVE_STOPWORDS and t in stops:\n",
        "            continue\n",
        "        if STEM:\n",
        "            t = stemmer.stem(t)\n",
        "        if LEMMATIZE:\n",
        "            t = lemmatizer.lemmatize(t)\n",
        "        processed.append(t)\n",
        "    return processed\n",
        "\n",
        "tokenized = [clean_tokenize(s) for s in sentences if len(s) > 0]\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(tokenized)\n",
        "\n",
        "split = int(0.8 * len(tokenized))\n",
        "split_valid = int(0.9 * len(tokenized))\n",
        "train_toks = tokenized[:split]\n",
        "valid_toks = tokenized[split:split_valid]\n",
        "test_toks = tokenized[split_valid:]\n",
        "\n",
        "# UNK treatment on train/valid/test\n",
        "word_counts = Counter(w for s in train_toks for w in s)\n",
        "def replace_unk(s, vocab):\n",
        "    return [w if w in vocab else '<UNK>' for w in s]\n",
        "\n",
        "if USE_UNK:\n",
        "    vocab = set([w for w, c in word_counts.items() if c > UNK_THRESH])\n",
        "    train_toks = [replace_unk(s, vocab) for s in train_toks]\n",
        "    valid_toks = [replace_unk(s, vocab) for s in valid_toks]\n",
        "    test_toks = [replace_unk(s, vocab) for s in test_toks]\n",
        "else:\n",
        "    vocab = set(w for s in train_toks for w in s)\n",
        "\n",
        "def add_tokens(data, n):\n",
        "    return [['<s>']*(n-1) + s + ['</s>'] for s in data]\n",
        "\n",
        "# Hyperparameter: select n by validation set perplexity\n",
        "def build_and_eval(n):\n",
        "    train_sents = add_tokens(train_toks, n)\n",
        "    valid_sents = add_tokens(valid_toks, n)\n",
        "    # Count ngrams\n",
        "    def count_ngrams(data, n):\n",
        "        ngrams, hist = Counter(), Counter()\n",
        "        for s in data:\n",
        "            for i in range(len(s) - n + 1):\n",
        "                ng = tuple(s[i:i+n])\n",
        "                h = ng[:-1]\n",
        "                ngrams[ng] += 1\n",
        "                hist[h] += 1\n",
        "        return ngrams, hist\n",
        "    c1, h1 = count_ngrams(train_sents, 1)\n",
        "    c2, h2 = count_ngrams(train_sents, 2)\n",
        "    c3, h3 = count_ngrams(train_sents, 3)\n",
        "    c4, h4 = count_ngrams(train_sents, 4)\n",
        "    V = len(vocab)\n",
        "    k = 0.1\n",
        "    def P1(w): return (c1.get((w,), 0) + k) / (sum(c1.values()) + k * V)\n",
        "    def P2(w1, w2): return (c2.get((w1, w2), 0) + k) / (h2.get((w1,), 0) + k * V)\n",
        "    def P3(w1, w2, w3): return (c3.get((w1, w2, w3), 0) + k) / (h3.get((w1, w2), 0) + k * V)\n",
        "    def P4(w1, w2, w3, w4): return (c4.get((w1, w2, w3, w4), 0) + k) / (h4.get((w1, w2, w3), 0) + k * V)\n",
        "    λ_bi, λ_tri, λ_quad = [0.1, 0.9], [0.1, 0.3, 0.6], [0.1, 0.2, 0.3, 0.4]\n",
        "    def interpolated_prob(n, hist, w):\n",
        "        if n == 2:\n",
        "            return λ_bi[0]*P1(w) + λ_bi[1]*P2(hist[-1], w)\n",
        "        elif n == 3:\n",
        "            return λ_tri[0]*P1(w) + λ_tri[1]*P2(hist[-1], w) + λ_tri[2]*P3(hist[-2], hist[-1], w)\n",
        "        elif n == 4:\n",
        "            return (λ_quad[0]*P1(w) + λ_quad[1]*P2(hist[-1], w) +\n",
        "                    λ_quad[2]*P3(hist[-2], hist[-1], w) + λ_quad[3]*P4(hist[-3], hist[-2], hist[-1], w))\n",
        "    def perplexity(n, data):\n",
        "        log_prob, N = 0, 0\n",
        "        for s in data:\n",
        "            for i in range(n-1, len(s)):\n",
        "                hist, w = s[i-n+1:i], s[i]\n",
        "                p = interpolated_prob(n, hist, w)\n",
        "                log_prob += math.log(p)\n",
        "                N += 1\n",
        "        return math.exp(-log_prob / N)\n",
        "    ppl = perplexity(n, valid_sents)\n",
        "    return ppl\n",
        "\n",
        "# Find n (2,3,4) that minimizes validation perplexity\n",
        "ppl_ns = {n: build_and_eval(n) for n in [2,3,4]}\n",
        "best_n = min(ppl_ns, key=ppl_ns.get)\n",
        "print(f\"Best n based on validation set: {best_n}\\nPerplexities: {ppl_ns}\")"
      ],
      "metadata": {
        "id": "UeDbOXcVZz14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Enter two starting words (e.g., 'the whale'):\")\n",
        "user_input = input(\">> \").lower().strip().split()\n",
        "\n",
        "print(\"\\n Sentence from Bigram:\")\n",
        "print(generate(2, user_input))\n",
        "\n",
        "print(\"\\n Sentence from Trigram:\")\n",
        "print(generate(3, user_input))\n",
        "\n",
        "print(\"\\n Sentence from 4-gram:\")\n",
        "print(generate(4, user_input))"
      ],
      "metadata": {
        "id": "A3lEvmA_ae3G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}